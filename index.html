<!DOCTYPE html>
<html>
<head>
  
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>OMG-Bench</title>
  <link rel="icon" type="image/x-icon" href="static/images/pinching-hand-noto-512.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title"><span style="color: #1F4E79;">OMG-Bench</span>: A New Challenging Benchmark for Skeleton-based Online 
Micro Hand Gesture Recognition</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                  <a href='https://omg-bench.github.io' target='_blank'>Haochen Chang</a><sup>1</sup>,</span>
                  <span class="author-block">
                    <a href='https://omg-bench.github.io' target='_blank'>Pengfei Ren</a><sup>2,*</sup>,</span>
                    <span class="author-block">
                      <a href='https://omg-bench.github.io' target='_blank'>Buyuan Zhang</a><sup>3</sup>,</span>
                      <span class="author-block">
                        <a href='https://omg-bench.github.io' target='_blank'>Da Li</a><sup>4</sup>,</span>
                        <span class="author-block">
                          <a href='https://omg-bench.github.io' target='_blank'>Tianhao Han</a><sup>3</sup>,</span>
                        <span class="author-block">
                          <a href='https://omg-bench.github.io' target='_blank'>Haoyang Zhang</a><sup>5,6</sup>,</span>
                        <span class="author-block">
                          <a href='https://omg-bench.github.io' target='_blank'>Liang Xie</a><sup>5,6</sup>,</span>
                        <span class="author-block">
                          <a href='https://omg-bench.github.io' target='_blank'>Hongbo Chen</a><sup>1</sup>,</span>
                                      <span class="author-block">
                          <a href='https://omg-bench.github.io' target='_blank'>Erwei Yin</a><sup>5,6,*</sup></span>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup>Sun Yat-sen University<br><sup>2</sup>Beijing University of Posts and Telecommunications<br><sup>3</sup>Shanghai Jiao Tong University<br><sup>4</sup>Nankai University<br><sup>5</sup>Academy of Military Sciences<br><sup>6</sup>Tianjin Artificial Intelligence Innovation Center</span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Corresponding Author</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2512.16727" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <!-- <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://omg-bench.github.io" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code (Coming soon)</span>
                  </a>
                </span>

                <!-- Dataset Link
                <span class="link-block">
                  <a href="https://omg-bench.github.io" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-google-drive"></i>
                  </span>
                  <span>Dataset</span>
                </a>
              </span> -->

                <!-- Dataset Link -->
                <span class="link-block">
                  <a href="https://omg-bench.github.io" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <!-- <i class="far fa-huggingface"></i> -->
                      <p style="font-size:18px">ðŸ¤—</p>
                  </span>
                  <span>Dataset (Coming soon)</span>
                </a>
              </span>
                
                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2512.16727" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>ArXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Youtube video -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <!-- Paper video. -->
      <h2 class="title is-3">Dataset and Application Demo</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="publication-video">
            <!-- Youtube embed code here -->
            <iframe src="https://www.youtube.com/embed/or6kilcybaU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
          <p>

           </p>          
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End youtube video -->


<!-- video demo -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Dataset and Application Demo</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="publication-video">
            <video controls width="100%" height="auto">
              <source src="static/videos/video_demo_arxiv.mp4" type="video/mp4">
            </video>
          </div>
        
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- video demo -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Online micro gesture recognition from hand skeletons is critical for VR/AR interaction but faces challenges due to limited public datasets and task-specific algorithms. Micro gestures involve subtle motion patterns, which make constructing datasets with precise skeletons and frame-level annotations difficult. To this end, we develop a multi-view self-supervised pipeline to automatically generate skeleton data, complemented by heuristic rules and expert refinement for semi-automatic annotation. Based on this pipeline, we introduce <strong style="color: #FF69B4; font-weight: bold;">OMG-Bench</strong>, the first large-scale public benchmark for skeleton-based online micro gesture recognition. It features 40 fine-grained gesture classes with 13,948 instances across 1,272 sequences, characterized by subtle motions, rapid dynamics, and continuous execution. To tackle these challenges, we propose <strong style="color: #FF69B4; font-weight: bold;">Hierarchical Memory-Augmented Transformer (HMATr)</strong>, an end-to-end framework that unifies gesture detection and classification by leveraging hierarchical memory banks which store frame-level details and window-level semantics to preserve historical context. In addition, it employs learnable position-aware queries initialized from the memory to implicitly encode gesture positions and semantics. Experiments show that HMATr outperforms state-of-the-art methods by 7.6\% in detection rate, establishing a strong baseline for online micro gesture recognition.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Data collection and annotation pipeline -->
<section class="section hero is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
          <h2 class="title is-3">Data collection and annotation pipeline</h2>
         <p>
          <img src="static\images\dataset.png" alt="Directional Weight Score" class="blend-img-background center-image" style="max-width: 100%; height: auto;" />
            </p>
            <p>
              Data collection and annotation pipeline of OMG-Bench, using a calibrated five-camera RGB-D system and self-supervised multi-view hand pose estimation to obtain high-quality skeletons, followed by semi-automatic frame-level gesture labeling.            </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Data collection and annotation pipeline -->
 
<!-- compare datasets -->
<section class="section hero is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
          <h2 class="title is-3">Comparison of Exisiting Datesets with Ours</h2>
         <p>
          <img src="static\images\datasets_compare.png" alt="Directional Weight Score" class="blend-img-background center-image" style="max-width: 100%; height: auto;" />
            </p>
            <p>
              Comparison between open-source skeleton-based gesture recognition datasets and the proposed OMG-Bench.
              </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- compare datasets -->

<!-- Dataset Properties -->
<section class="section hero is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
          <h2 class="title is-3">Dataset Properties </h2>
         <p>
          <img src="static\images\properties.png" alt="Directional Weight Score" class="blend-img-background center-image" style="max-width: 100%; height: auto;" />
            </p>
            <p>
             (a) Types and locations of defined micro gestures. TIP, PIP, and MCP denote the fingertip, proximal interphalangeal joint, and metacarpophalangeal joint. (b) Statistics of gesture types. (c) Distribution of sample counts per class.            </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- Dataset Properties -->



<!-- Method -->
<section class="section hero is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
          <h2 class="title is-3">Method </h2>
         <p>
          <img src="static\images\method.png" alt="Directional Weight Score" class="blend-img-background center-image" style="max-width: 100%; height: auto;" />
            </p>
            <p>
              <strong style="color: #FF69B4; font-weight: bold;">Overview of our proposed HMATr.</strong> (a) Lightweight backbone processes streaming skeleton inputs using a non-overlapping sliding window approach. (b) Hierarchical memory bank uses historical temporal information to enrich the content of the current window. (c) Position-aware queries implicitly capture potential hand movements, enabling unified detection and recognition. (d) Memory Interaction and Position-aware Interaction encode both position and semantic information of gesture instances from the memory-enhanced features.
            </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- Method -->

<!-- Benchmark Evaluation -->
<section class="section hero is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
          <h2 class="title is-3">OMG-Bench Benchmark Evaluation</h2>
         <p>
          <img src="static\images\benchmark1.png" alt="Directional Weight Score" class="blend-img-background center-image" style="max-width: 100%; height: auto;" />
            </p>
            <p>
              Benchmark of SOTA methods with four metrics. Methods marked with * denote our re-implementations owing to the absence of open-source code.
            </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- Benchmark Evaluation -->


<!-- Qualitative Results -->
<section class="section hero is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
          <h2 class="title is-3">Qualitative Results</h2>
         <p>
          <img src="static\images\exper_vis.png" alt="Directional Weight Score" class="blend-img-background center-image" style="max-width: 100%; height: auto;" />
            </p>
            <p>
              <strong style="color: #FF69B4; font-weight: bold;">(a) Query Distribution.</strong> We present the distribution of query relative positions within the window for all test samples. We observe that different queries tend to focus on different positions, thereby capturing diverse types of micro-gesture features. This enhances the feature representation capability of HMATr. <strong style="color: #FF69B4; font-weight: bold;">(b) Comparison of Gesture Segment Detection Results.</strong> We visualize the micro gesture detection results of different methods. The results demonstrate that our method can accurately identify the boundaries of consecutive same-class gestures, whereas the other two methods tend to either merge multiple consecutive same-class gestures or over-segment them. This highlights the superiority of our approach.
            </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- Qualitative Results -->




<!-- Visualization of our methodâ€™s results -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Visualization of our methodâ€™s results</h2>

      <iframe  src="static/pdfs/sup_ours_results.pdf" width="100%" height="550">
          </iframe>
      <iframe  src="static/pdfs/sup_ours_results1.pdf" width="100%" height="550">
          </iframe>        
      </div>
    </div>
  </section> -->
<!--Visualization of our methodâ€™s results -->

<!-- Youtube video -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container"> -->
      <!-- Paper video. -->
      <!-- <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video"> -->
            <!-- Youtube embed code here -->
            <!-- <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End youtube video -->


<!-- Video carousel -->
<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Another Carousel</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop height="100%"> -->
            <!-- Your video file here -->
            <!-- <source src="static/videos/carousel1.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop height="100%"> -->
            <!-- Your video file here -->
            <!-- <source src="static/videos/carousel2.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop height="100%">\ -->
            <!-- Your video file here -->
            <!-- <source src="static/videos/carousel3.mp4"
            type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div> -->
<!-- </section> -->
<!-- End video carousel -->






<!-- Paper poster -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section> -->
<!--End paper poster -->


<!-- BibTex citation -->
  <!-- <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{zhong2025dexgrasp,
        title={DexGrasp Anything: Towards Universal Robotic Dexterous Grasping with Physics Awareness},
        author={Zhong, Yiming and Jiang, Qi and Yu, Jingyi and Ma, Yuexin},
        journal={arXiv preprint arXiv:2503.08257},
        year={2025}
      }</code></pre>
    </div>
</section> -->
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from theÂ <a href="https://nerfies.github.io" target="_blank">Nerfies</a>Â project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
